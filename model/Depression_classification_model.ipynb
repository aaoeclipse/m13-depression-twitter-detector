{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAwhrjdoE8yT"
      },
      "source": [
        "##Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iMxZfiZLGOmf"
      },
      "outputs": [],
      "source": [
        "EnglishShortened_path = 'content/EnglishShortened.csv'\n",
        "\n",
        "sentimental_depressed_path = 'content/sentimental_depressed.csv'\n",
        "sentimental_non_depressed_path = 'content/sentimental_non_depressed.csv'\n",
        "\n",
        "non_depressed_tweets_path = 'content/non_depressed_tweets.csv'\n",
        "depressed_tweets_path = 'content/depressed_tweets.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQUxqaeHES9s"
      },
      "source": [
        "Import all necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I11lnjKwEI3e",
        "outputId": "9f01744e-5424-4706-865f-9ff8d2c22d5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/eclipse/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/eclipse/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /home/eclipse/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/eclipse/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/eclipse/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to\n",
            "[nltk_data]     /home/eclipse/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hli5PTLFOxsr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh9x6FjWEWcJ"
      },
      "source": [
        "Function to clean tweet (remove all punctuations, abbreviations of words, capital letters, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tP2v5ZbYEQ2Z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "  tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
        "  \n",
        "  tweet = tweet.replace(\".\", \"\")\n",
        "  tweet = tweet.replace(\"(\", \"\")\n",
        "  tweet = tweet.replace(\")\", \"\")\n",
        "  tweet = tweet.replace(\"'m\", \" am\")\n",
        "  tweet = tweet.replace(\"'s\", \" is\")\n",
        "  tweet = tweet.replace(\"'ve\", \" have\")\n",
        "  tweet = tweet.replace(\"n't\", \" not\")\n",
        "  tweet = tweet.replace(\"'re\", \" are\")\n",
        "  tweet = tweet.replace(\"'d\", \" would\")\n",
        "  tweet = tweet.replace(\"'ll\", \" will\")\n",
        "  tweet = tweet.replace(\"\\r\", \" \")\n",
        "  tweet = tweet.replace(\"\\n\", \" \")\n",
        "  tweet = tweet.strip().lower()\n",
        "  \n",
        "  tweet = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", tweet)\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvJc80HaEops"
      },
      "source": [
        "##Metric functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J8TPC1gNEoD_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "def find_words_frequency(tweet, words_dict):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  token = re.findall('\\w+', tweet_clean)\n",
        "  words_freq = nltk.FreqDist(token)\n",
        "  total_freq = 0\n",
        "  for item in words_dict:\n",
        "    item_freq = words_freq.freq(item)\n",
        "    total_freq += item_freq\n",
        "  return total_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwBm_8ycFDPi"
      },
      "source": [
        "Amount of absolutist words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fBprf4CiFHnA"
      },
      "outputs": [],
      "source": [
        "def absolutist_words_metric(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  absolutist_words_dict = [\"absolutely\", \"all\", \"always\", \"complete\", \"completely\", \"constant\", \"constantly\", \"definitely\", \"entire\",\n",
        "                           \"ever\", \"every\", \"everyone\", \"everything\", \"full\", \"must\", \"never\", \"nothing\", \"totally\", \"whole\"]\n",
        "  return find_words_frequency(tweet_clean, absolutist_words_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgdc2O0QFR7J"
      },
      "source": [
        "Amount of first-person pronouns (increases with depression) + amount of second & third pronouns (decreases with depression) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dEn9wvYEFJqs"
      },
      "outputs": [],
      "source": [
        "def first_pronouns_metric(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  first_person_pronoun_dict = [\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"]\n",
        "  return find_words_frequency(tweet_clean, first_person_pronoun_dict)\n",
        "\n",
        "def second_third_pronouns_metric(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  second_third_person_pronoun_dict = [\"you\", \"your\", \"yours\",\n",
        "                                      \"he\", \"she\", \"it\", \"him\", \"her\", \"his\", \"its\", \"hers\",\n",
        "                                      \"they\", \"them\", \"their\", \"theirs\"]\n",
        "  return find_words_frequency(tweet_clean, second_third_person_pronoun_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVIxrFGaFTNc"
      },
      "source": [
        "Polarity + subjectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F4DVfHcUFMXJ"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def TextBlob_metrics(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  blob = TextBlob(tweet_clean)\n",
        "  for sentence in blob.sentences:\n",
        "    polarity = sentence.sentiment.polarity\n",
        "    subjectivity = sentence.sentiment.subjectivity\n",
        "  return polarity, subjectivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeYfOZj_Fawb"
      },
      "source": [
        "Level of emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cz5Z-Kh2Fbem"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_anew_dict():\n",
        "  anew_dict = {}\n",
        "  with open(EnglishShortened_path, mode='r') as infile:\n",
        "    reader = csv.reader(infile)\n",
        "    anew_dict = {rows[0]:[rows[1], rows[2], rows[3]] for rows in reader}\n",
        "  return anew_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0EW1XsaFFbgj"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_tweet(tweet_clean):\n",
        "  token = re.findall('\\w+', tweet_clean)\n",
        "  lemmatized_words = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for word in token:\n",
        "    lemmatized_words.append(lemmatizer.lemmatize(word))\n",
        "  return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zohJ9IVDFMY4"
      },
      "outputs": [],
      "source": [
        "def anew_metric(tweet):\n",
        "  anew_dict = create_anew_dict()\n",
        "  valence, arousal, dominance = 0, 0, 0\n",
        "   \n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  tweet_words = lemmatize_tweet(tweet_clean)\n",
        "  N_words_total  = len(tweet_words) \n",
        "   \n",
        "  for index in range(N_words_total):\n",
        "    # check for negation in 3 words before current word\n",
        "    j = index-1\n",
        "    neg = False\n",
        "    while j >= 0 and j >= index-3:\n",
        "      if tweet_words[j] == 'not' or tweet_words[j] == 'no':\n",
        "        neg = True\n",
        "        break\n",
        "      j -= 1\n",
        "\n",
        "    # search for lemmatized word in ANEW\n",
        "    if tweet_words[index] in anew_dict.keys():\n",
        "      if neg:\n",
        "        valence += float(anew_dict[tweet_words[index]][0])\n",
        "        arousal += float(anew_dict[tweet_words[index]][1])\n",
        "        dominance +=  float(anew_dict[tweet_words[index]][2])\n",
        "      else:\n",
        "        valence += (10 - float(anew_dict[tweet_words[index]][0]))\n",
        "        arousal += (10 - float(anew_dict[tweet_words[index]][1]))\n",
        "        dominance += (10 - float(anew_dict[tweet_words[index]][2]))\n",
        "\n",
        "  if N_words_total == 0:\n",
        "    return 0, 0, 0\n",
        "  else:\n",
        "    return valence/N_words_total, arousal/N_words_total, dominance/N_words_total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obfHrXX4F0_C"
      },
      "source": [
        "Ratio of pronouns to nouns, Ratio of verbs to nouns, Ratio of the number of punctuation to the number of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gGcPYypCF4kT"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def pronominalisation_index(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  tokens = re.findall('\\w+', tweet_clean)\n",
        "  tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "  tags_freq = nltk.FreqDist(tag for (word, tag) in tags)\n",
        "  if tags_freq['NOUN'] == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return tags_freq['PRON']/tags_freq['NOUN']\n",
        "\n",
        "def readiness_to_action_index(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  tokens = re.findall('\\w+', tweet_clean)\n",
        "  tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "  tags_freq = nltk.FreqDist(tag for (word, tag) in tags)\n",
        "  if tags_freq['NOUN'] == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return tags_freq[\"VERB\"]/tags_freq[\"NOUN\"]\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "\n",
        "def punctuation_metric(tweet):\n",
        "  count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
        "  num_punct = count(tweet,set(string.punctuation))          \n",
        "  num_sentences = len(sent_tokenize(tweet))\n",
        "  if num_sentences == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return num_punct/num_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwFM3Je8GDwU"
      },
      "source": [
        "Get vector based on all metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yXbjG6Y1F_HM"
      },
      "outputs": [],
      "source": [
        "def get_tweet_vector(tweet):\n",
        "  weight_abolutist_metric = 4\n",
        "  weight_pronouns_metric = 2\n",
        "  weight_textblob = 1\n",
        "  weight_anew_metric = 1\n",
        "  weight_pos_metric = 1\n",
        "\n",
        "  absolutist = [absolutist_words_metric(tweet)]*weight_abolutist_metric\n",
        "  pronouns = [first_pronouns_metric(tweet)]*weight_pronouns_metric + [second_third_pronouns_metric(tweet)]*weight_pronouns_metric\n",
        "  textblob = [TextBlob_metrics(tweet)[0]]*weight_textblob + [TextBlob_metrics(tweet)[1]]*weight_textblob\n",
        "  anew = [anew_metric(tweet)[0]]*weight_anew_metric + [anew_metric(tweet)[1]]*weight_anew_metric + [anew_metric(tweet)[2]]*weight_anew_metric\n",
        "  pos = [pronominalisation_index(tweet)]*weight_pos_metric + [readiness_to_action_index(tweet)]*weight_pos_metric + [punctuation_metric(tweet)]*weight_pos_metric\n",
        "\n",
        "  tweet_vector = absolutist + pronouns + textblob + anew + pos\n",
        "  \n",
        "  return tweet_vector               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rKsaRaAjF_Iz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import ast\n",
        "\n",
        "def get_tweet_data_vector(tweet_vector, timestamp, likes, retweets, sentimental):\n",
        "  timestamp = timestamp[:19]\n",
        "  timestamp = datetime.datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
        "  time_vector =  [timestamp.month, timestamp.day, timestamp.hour, timestamp.minute]\n",
        "\n",
        "  tweet_vector = ast.literal_eval(str(tweet_vector))\n",
        "  \n",
        "  metric = tweet_vector + time_vector + [likes, retweets] + [sentimental]*5\n",
        "  return metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nMTheRlHHp3"
      },
      "source": [
        "##Data preparation for ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P_ASsV4BF_qW"
      },
      "outputs": [],
      "source": [
        "#read files\n",
        "sentimental_depressed = pd.read_csv(sentimental_depressed_path)\n",
        "sentimental_non_depressed = pd.read_csv(sentimental_non_depressed_path)\n",
        "\n",
        "depressed_tweets = pd.read_csv(depressed_tweets_path)\n",
        "non_depressed_tweets = pd.read_csv(non_depressed_tweets_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OQb6uWmNHId-"
      },
      "outputs": [],
      "source": [
        "depressed_tweets['tweet_vector'] = depressed_tweets['tweet'].apply(get_tweet_vector)\n",
        "non_depressed_tweets['tweet_vector'] = non_depressed_tweets['tweet'].apply(get_tweet_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wgmu5KE2lU08"
      },
      "outputs": [],
      "source": [
        "#add sentimental_value column to data frames\n",
        "depressed_tweets['sentimental_value'] = sentimental_depressed['0']\n",
        "non_depressed_tweets['sentimental_value'] = sentimental_non_depressed\n",
        "#create status column to separate depressed and non-depressed cases\n",
        "depressed_tweets['status'] = 1\n",
        "non_depressed_tweets['status'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LW26P9fWKo06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_87470/3587298165.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_tweets = depressed_tweets.append(non_depressed_tweets, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "#merge dataframes with depressed and non depressed tweets\n",
        "all_tweets = depressed_tweets.append(non_depressed_tweets, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Uezgwk1sHIgj"
      },
      "outputs": [],
      "source": [
        "#create vector for each tweet based on metrics + timestamp + sentimental analysis + amount of likes and retweets\n",
        "all_tweets['tweet_vector'] = all_tweets.apply(lambda x: get_tweet_data_vector(x.tweet_vector, x.timestamp, x.favorite_count, x.retweet_count, x.sentimental_value), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZSHM8mBAHIjR"
      },
      "outputs": [],
      "source": [
        "#cretating several columns for tweet vector values\n",
        "all_tweets['tweet_vector'] = all_tweets.apply(lambda x: ast.literal_eval(str(x.tweet_vector)), axis='columns')\n",
        "all_tweets = all_tweets.join(all_tweets['tweet_vector'].apply(pd.Series))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CWQBjuA9K_98"
      },
      "outputs": [],
      "source": [
        "#calculate average for one user tweets\n",
        "all_tweets_average = pd.DataFrame(all_tweets.groupby('userID').mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iJnt3QML0lq"
      },
      "source": [
        "##Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYBls9vuLU4k"
      },
      "source": [
        "Prepare dataset for classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QFK5mSN7L6Lo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm, tree\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FCtFVxKpK__8"
      },
      "outputs": [],
      "source": [
        "X  = all_tweets_average[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]\n",
        "y = all_tweets_average['status']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vboHGirELACC"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5PiUNoZL33t"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YZFB40vQLgjK"
      },
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(random_state=0, max_features='auto', n_estimators=500, max_depth=8, criterion='entropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvX2lvceLjvL",
        "outputId": "e856cead-5e90-4b7c-c0e1-f7afa551faa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eclipse/Documents/HarborSpace/Industrial/Project/ml_model_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Random Forest Classifier is 0.978515625\n"
          ]
        }
      ],
      "source": [
        "rfc.fit(X_train, y_train)\n",
        "y_pred= rfc.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Random Forest Classifier is %s\"%acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7h1885pLp1a",
        "outputId": "4686105d-aa09-4f85-b6ff-3702ab8d8872"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eclipse/Documents/HarborSpace/Industrial/Project/ml_model_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/home/eclipse/Documents/HarborSpace/Industrial/Project/ml_model_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/home/eclipse/Documents/HarborSpace/Industrial/Project/ml_model_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/home/eclipse/Documents/HarborSpace/Industrial/Project/ml_model_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/home/eclipse/Documents/HarborSpace/Industrial/Project/ml_model_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'fit_time': array([2.06772327, 2.10412908, 2.08652258, 2.06098294, 2.06627655]),\n",
              " 'score_time': array([0.05565143, 0.05115366, 0.04776049, 0.05018139, 0.04881406]),\n",
              " 'test_score': array([0.98046875, 0.96484375, 0.96868885, 0.9667319 , 0.98238748])}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cross_validate(rfc, X, y, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_nw66WALstq",
        "outputId": "60430f36-0718-4706-953f-634913d7e5c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[9.94085324e-01, 5.91467638e-03],\n",
              "       [9.99940573e-01, 5.94268477e-05],\n",
              "       [8.35223394e-01, 1.64776606e-01],\n",
              "       ...,\n",
              "       [9.97875356e-01, 2.12464417e-03],\n",
              "       [9.79178588e-01, 2.08214125e-02],\n",
              "       [1.00680950e-01, 8.99319050e-01]])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfc.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pickle' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb#ch0000044?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../web/model/model_pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m files:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb#ch0000044?line=1'>2</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(rfc, files)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ],
      "source": [
        "with open('../web/model/model_pkl', 'wb') as files:\n",
        "    pickle.dump(rfc, files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEo_axp1hSll"
      },
      "source": [
        "#One test user scenario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbfHwNTTrNi6"
      },
      "source": [
        "As input we have dataframe with 20 rows (tweets) and next columns: userID, lang (only english (en)), tweet, timestamp, favorite_count, retweet_count.\n",
        "\n",
        "We need to do implement sentimental analysis for this tweets and get another dataframe with 1 column and value of sentimental analysis result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIw1jt84hR7-"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb#ch0000047?line=0'>1</a>\u001b[0m user_tweets_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/content/jesseayye.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb#ch0000047?line=1'>2</a>\u001b[0m sentimental_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/content/sent_jesseayye.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb#ch0000047?line=3'>4</a>\u001b[0m tweets \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(user_tweets_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eclipse/Documents/HarborSpace/Industrial/Project/model/Depression_classification_model.ipynb#ch0000047?line=4'>5</a>\u001b[0m sentimental \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(sentimental_path)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "user_tweets_path = \"/content/jesseayye.csv\"\n",
        "sentimental_path = \"/content/sent_jesseayye.csv\"\n",
        "\n",
        "tweets = pd.read_csv(user_tweets_path)\n",
        "sentimental = pd.read_csv(sentimental_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grj9be71sEzR"
      },
      "source": [
        "Calculate metrics for each tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciXeUx-LsD9S"
      },
      "outputs": [],
      "source": [
        "#get column with list of metrics\n",
        "tweets['tweet_vector'] = tweets['tweet'].apply(get_tweet_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkPxStBYsloo"
      },
      "outputs": [],
      "source": [
        "#add sentimental analysis value\n",
        "tweets['sentimental_value'] = sentimental['0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDXBb2M3swuJ"
      },
      "outputs": [],
      "source": [
        "#get vector based on metric vector + timestamp, sentimental analysis, likes and retweets\n",
        "tweets['tweet_vector'] = tweets.apply(lambda x: get_tweet_data_vector(x.tweet_vector, x.timestamp, x.favorite_count, x.retweet_count, x.sentimental_value), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRLQMDbUs9te"
      },
      "outputs": [],
      "source": [
        "#cretae several columns for each vector value\n",
        "tweets['tweet_vector'] = tweets.apply(lambda x: ast.literal_eval(str(x.tweet_vector)), axis='columns')\n",
        "tweets = tweets.join(tweets['tweet_vector'].apply(pd.Series))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvbIvP9etedQ"
      },
      "outputs": [],
      "source": [
        "#count average values for this user\n",
        "tweets_average = pd.DataFrame(tweets.groupby('userID').mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewf3c6PxtsgI"
      },
      "source": [
        "Fit classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmirmy4Ttzgk"
      },
      "outputs": [],
      "source": [
        "X = tweets_average[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub5KtHwBtvDK"
      },
      "outputs": [],
      "source": [
        "depr_prob = rfc.predict_proba(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HAnYb5svHvV",
        "outputId": "0976e1e4-2ae5-450f-d572-694058154eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Depression probability: 0.53%\n"
          ]
        }
      ],
      "source": [
        "print(\"Depression probability: \" + str(round(depr_prob[0][1]*100, 2)) + \"%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Depression_classification_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ml_model_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "5b8ca6201ae4462137b17dfbba56dae4bc3de67c36a81b774b87464d0ec76df8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
