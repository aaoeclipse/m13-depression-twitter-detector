{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19rjUvUr3GMP"
      },
      "outputs": [],
      "source": [
        "tweet = '''abandon @OutofAlcatraz I am diagnosed no with borderline personality disorder, obsessive compulsive disorder, post traumatic stress disorder, major depression and ed. I am fucked up too. Who cares? I care. I accept the situation and atleast give some shots to fight them'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbU0sSjRJ-Qu",
        "outputId": "d0fea55a-c854-4244-fabe-24824b3e14a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeDbJe5p3URn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "  tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
        "  \n",
        "  tweet = tweet.replace(\".\", \"\")\n",
        "  tweet = tweet.replace(\"(\", \"\")\n",
        "  tweet = tweet.replace(\")\", \"\")\n",
        "  tweet = tweet.replace(\"'m\", \" am\")\n",
        "  tweet = tweet.replace(\"'s\", \" is\")\n",
        "  tweet = tweet.replace(\"'ve\", \" have\")\n",
        "  tweet = tweet.replace(\"n't\", \" not\")\n",
        "  tweet = tweet.replace(\"'re\", \" are\")\n",
        "  tweet = tweet.replace(\"'d\", \" would\")\n",
        "  tweet = tweet.replace(\"'ll\", \" will\")\n",
        "  tweet = tweet.replace(\"\\r\", \" \")\n",
        "  tweet = tweet.replace(\"\\n\", \" \")\n",
        "  tweet = tweet.strip().lower()\n",
        "  \n",
        "  tweet = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", tweet)\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkYlAWDHG1Bz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "def find_words_frequency(tweet, words_dict):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  token = re.findall('\\w+', tweet_clean)\n",
        "  words_freq = nltk.FreqDist(token)\n",
        "  total_freq = 0\n",
        "  for item in words_dict:\n",
        "    item_freq = words_freq.freq(item)\n",
        "    total_freq += item_freq\n",
        "  return total_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHHf3d235dXA"
      },
      "outputs": [],
      "source": [
        "def absolutist_words_metric(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  absolutist_words_dict = [\"absolutely\", \"all\", \"always\", \"complete\", \"completely\", \"constant\", \"constantly\", \"definitely\", \"entire\",\n",
        "                           \"ever\", \"every\", \"everyone\", \"everything\", \"full\", \"must\", \"never\", \"nothing\", \"totally\", \"whole\"]\n",
        "  return find_words_frequency(tweet_clean, absolutist_words_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKfKIDUFFNRg"
      },
      "outputs": [],
      "source": [
        "def first_pronouns_metric(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  first_person_pronoun_dict = [\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"]\n",
        "  return find_words_frequency(tweet_clean, first_person_pronoun_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmfpxVvZF_6f"
      },
      "outputs": [],
      "source": [
        "def second_third_pronouns_metric(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  second_third_person_pronoun_dict = [\"you\", \"your\", \"yours\",\n",
        "                                      \"he\", \"she\", \"it\", \"him\", \"her\", \"his\", \"its\", \"hers\",\n",
        "                                      \"they\", \"them\", \"their\", \"theirs\"]\n",
        "  return find_words_frequency(tweet_clean, second_third_person_pronoun_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9R5MbMbNX-Y"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def TextBlob_metrics(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  blob = TextBlob(tweet_clean)\n",
        "  for sentence in blob.sentences:\n",
        "    polarity = sentence.sentiment.polarity\n",
        "    subjectivity = sentence.sentiment.subjectivity\n",
        "  return polarity, subjectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfI-wgXjVjAE"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_anew_dict():\n",
        "  anew_dict = {}\n",
        "  with open('/content/EnglishShortened.csv', mode='r') as infile:\n",
        "    reader = csv.reader(infile)\n",
        "    anew_dict = {rows[0]:[rows[1], rows[2], rows[3]] for rows in reader}\n",
        "  return anew_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQVVS-wGakLe"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_tweet(tweet_clean):\n",
        "  token = re.findall('\\w+', tweet_clean)\n",
        "  lemmatized_words = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for word in token:\n",
        "    lemmatized_words.append(lemmatizer.lemmatize(word))\n",
        "  return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnBdmCBcTW-W"
      },
      "outputs": [],
      "source": [
        "def anew_metric(tweet):\n",
        "  anew_dict = create_anew_dict()\n",
        "  valence, arousal, dominance = 0, 0, 0\n",
        "   \n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  tweet_words = lemmatize_tweet(tweet_clean)\n",
        "  N_words_total  = len(tweet_words) \n",
        "   \n",
        "  for index in range(N_words_total):\n",
        "    # check for negation in 3 words before current word\n",
        "    j = index-1\n",
        "    neg = False\n",
        "    while j >= 0 and j >= index-3:\n",
        "      if tweet_words[j] == 'not' or tweet_words[j] == 'no':\n",
        "        neg = True\n",
        "        break\n",
        "      j -= 1\n",
        "\n",
        "    # search for lemmatized word in ANEW\n",
        "    if tweet_words[index] in anew_dict.keys():\n",
        "      if neg:\n",
        "        valence += float(anew_dict[tweet_words[index]][0])\n",
        "        arousal += float(anew_dict[tweet_words[index]][1])\n",
        "        dominance +=  float(anew_dict[tweet_words[index]][2])\n",
        "      else:\n",
        "        valence += (10 - float(anew_dict[tweet_words[index]][0]))\n",
        "        arousal += (10 - float(anew_dict[tweet_words[index]][1]))\n",
        "        dominance += (10 - float(anew_dict[tweet_words[index]][2]))\n",
        "\n",
        "  return valence/N_words_total, arousal/N_words_total, dominance/N_words_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4ngKtCveGXR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def pronominalisation_index(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  tokens = re.findall('\\w+', tweet_clean)\n",
        "  tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "  tags_freq = nltk.FreqDist(tag for (word, tag) in tags)\n",
        "  return tags_freq['PRON']/tags_freq['NOUN']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N-33rj_m331"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def readiness_to_action_index(tweet):\n",
        "  tweet_clean = clean_tweet(tweet)\n",
        "  tokens = re.findall('\\w+', tweet_clean)\n",
        "  tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "  tags_freq = nltk.FreqDist(tag for (word, tag) in tags)\n",
        "  return tags_freq[\"VERB\"]/tags_freq[\"NOUN\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rWpLkM2nMHU"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "\n",
        "def punctuation_metric(tweet):\n",
        "  count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
        "  num_punct = count(tweet,set(string.punctuation))          \n",
        "  num_sentences = len(sent_tokenize(tweet))\n",
        "  return num_punct/num_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU1qrLt3XGIL"
      },
      "outputs": [],
      "source": [
        "def get_tweet_vector(tweet):\n",
        "  weight_abolutist_metric = 4\n",
        "  weight_pronouns_metric = 2\n",
        "  weight_textblob = 1\n",
        "  weight_anew_metric = 1\n",
        "  weight_pos_metric = 1\n",
        "\n",
        "  absolutist = [absolutist_words_metric(tweet)]*weight_abolutist_metric\n",
        "  pronouns = [first_pronouns_metric(tweet)]*weight_pronouns_metric + [second_third_pronouns_metric(tweet)]*weight_pronouns_metric\n",
        "  textblob = [TextBlob_metrics(tweet)[0]]*weight_textblob + [TextBlob_metrics(tweet)[1]]*weight_textblob\n",
        "  anew = [anew_metric(tweet)[0]]*weight_anew_metric + [anew_metric(tweet)[1]]*weight_anew_metric + [anew_metric(tweet)[2]]*weight_anew_metric\n",
        "  pos = [pronominalisation_index(tweet)]*weight_pos_metric + [readiness_to_action_index(tweet)]*weight_pos_metric + [punctuation_metric(tweet)]*weight_pos_metric\n",
        "\n",
        "  tweet_vector = absolutist + pronouns + textblob + anew + pos\n",
        "  \n",
        "  return tweet_vector               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJwgEB_TYtP7",
        "outputId": "502a708a-4b37-4952-d54f-e8b490d93b89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.09523809523809523,\n",
              " 0.09523809523809523,\n",
              " 0.023809523809523808,\n",
              " 0.023809523809523808,\n",
              " -0.26875,\n",
              " 0.6,\n",
              " 2.6204761904761904,\n",
              " 2.5438095238095237,\n",
              " 2.566190476190476,\n",
              " 0.14285714285714285,\n",
              " 0.7142857142857143,\n",
              " 1.6]"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_tweet_vector(tweet)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Metrics_twitter_depression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('webapp': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "93a80c017611ce296236bca388b0e16c3c090e39fa5885d1c9b6367f0ed36b63"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
